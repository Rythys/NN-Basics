{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI7esNpvy3lE"
      },
      "source": [
        "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSosa1i9y3lE"
      },
      "source": [
        "Содержиние:\n",
        "1. Реализация прямого вывода нейронной сети\n",
        "2. Реализация градиентов по входу и распространения градиента по сети\n",
        "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети\n",
        "\n",
        "В дальнейшем ланируется реализация обучения сети со свёрточными слоями, с транспонированной свёрткой, дополнительного оптимизатора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4ah7Okby3lE"
      },
      "source": [
        "###  1. Реализация вывода собственной нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBBMu36Ey3lF"
      },
      "source": [
        "1.1 Любой слой содержит как минимум три метода:\n",
        "- конструктор\n",
        "- прямой вывод\n",
        "- обратный вывод, производные по входу и по параметрам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0VegAV5y3lF"
      },
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.name = 'Layer'\n",
        "    def forward(self, input_data):\n",
        "        pass\n",
        "    def backward(self, input_data):\n",
        "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        pass\n",
        "    def grad_param(self, input_data):\n",
        "        return []\n",
        "\n",
        "    def update_param(self, grads, learning_rate):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGYJvkMty3lF"
      },
      "source": [
        "1.2 Ниже предствален интерфейс класса  Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1wihmH4y3lG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, layers, loss=None):\n",
        "        self.name = 'Network'\n",
        "        self.layers = layers\n",
        "        self.loss = loss\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return self.predict(input_data)\n",
        "\n",
        "    def grad_x(self, input_data, labels):\n",
        "        out_tup = []\n",
        "        curr_data = input_data\n",
        "        for layer in self.layers:\n",
        "            curr_grad = layer.grad_x(curr_data)\n",
        "            curr_data = layer.forward(curr_data)\n",
        "            # print(curr_grad.shape)\n",
        "            out_tup.append(curr_grad)\n",
        "        loss_grad = self.loss.grad_x(curr_data,labels)\n",
        "        out_tup.append(loss_grad)\n",
        "        tup = []\n",
        "        tup.append(np.transpose(out_tup[0],(0,2,1)))\n",
        "        for i in range(1,len(out_tup)):\n",
        "          tmp = []\n",
        "          for j in range(out_tup[i].shape[0]):\n",
        "            tmp.append(np.dot(tup[-1][j],np.transpose(out_tup[i][j])))\n",
        "          tup.append(np.array(tmp))\n",
        "        return tup[-1]\n",
        "    def grad_param(self, input_data, labels):\n",
        "        grads = []\n",
        "        outs = []\n",
        "        curr_data = input_data\n",
        "        outs.append(curr_data)\n",
        "        for layer in self.layers:\n",
        "            grads.append(layer.grad_x(curr_data))\n",
        "            curr_data = layer.forward(curr_data)\n",
        "            outs.append(curr_data)\n",
        "        return grads, outs\n",
        "\n",
        "    def update(self, grad_list, learning_rate):\n",
        "      for i in range(len(self.layers)):\n",
        "        self.layers[i].update_param(grad_list[i], learning_rate)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        current_input = input_data\n",
        "        for layer in self.layers:\n",
        "            current_input = layer.forward(current_input)\n",
        "        return current_input\n",
        "\n",
        "    def calculate_loss(self, input_data, labels):\n",
        "        return self.loss.forward(self.predict(input_data), labels)\n",
        "\n",
        "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
        "        batch = input_data.shape[0]\n",
        "        grads, outs = self.grad_param(input_data, labels)\n",
        "        # print(len(grads), len(outs), len(self.layers))\n",
        "        loss_grad = self.loss.grad_x(outs[-1], labels)\n",
        "        thru_grads = []\n",
        "\n",
        "        thru_grad = loss_grad\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            if self.layers[i].name == 'Dense':\n",
        "              params_grad = self.layers[i].grad_param(outs[i])\n",
        "              full_params = []\n",
        "              tmp_W = []\n",
        "              tmp_b = []\n",
        "              for j in range(batch):\n",
        "                tmp_W.append(np.dot(thru_grad[j], params_grad[0][j]))\n",
        "                tmp_b.append(thru_grad[j])\n",
        "              full_params.append(np.array(tmp_W))\n",
        "              full_params.append(np.array(tmp_b))\n",
        "              self.layers[i].update_param(full_params, learning_rate)\n",
        "            tmp_grad = []\n",
        "            for j in range(batch):\n",
        "              tmp_grad.append(np.dot(np.transpose(grads[i][j]), thru_grad[j]))\n",
        "            thru_grad = np.array(tmp_grad)\n",
        "\n",
        "    def fit(self, trainX, trainY, validation_split=0.25,\n",
        "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
        "\n",
        "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY,\n",
        "                                                          test_size=validation_split,\n",
        "                                                          random_state=42)\n",
        "        for epoch in range(nb_epoch):\n",
        "            #train one epoch\n",
        "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
        "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
        "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
        "                self.train_step(batch_x, batch_y, learning_rate)\n",
        "            #validate\n",
        "            val_accuracy = self.evaluate(val_x, val_y)\n",
        "            print('%d epoch: val %.2f' %(epoch+1, val_accuracy))\n",
        "\n",
        "    def evaluate(self, testX, testY):\n",
        "        y_pred = np.argmax(self.predict(testX), axis=1)\n",
        "        y_true = np.argmax(testY, axis=1)\n",
        "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
        "        return val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD5m-_aPy3lG"
      },
      "source": [
        "#### 1.1 Реализация метода forward для вычисления следующих слоёв:\n",
        "\n",
        "- DenseLayer\n",
        "- ReLU\n",
        "- Softmax\n",
        "- FlattenLayer\n",
        "- MaxPooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJw7mszfy3lG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ReSbwo1y3lG"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(Layer):\n",
        "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
        "        self.name = 'Dense'\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        if W_init is None or b_init is None:\n",
        "            self.W = np.random.random((input_dim, output_dim))\n",
        "            self.b = np.zeros(output_dim, 'float32')\n",
        "        else:\n",
        "            self.W = W_init\n",
        "            self.b = b_init\n",
        "    def forward(self, input_data):\n",
        "        shape = input_data.shape\n",
        "        out = np.zeros((shape[0],self.output_dim))\n",
        "        out = np.dot(input_data,self.W) + self.b\n",
        "        return out\n",
        "    def grad_x(self, input_data):\n",
        "        out = np.zeros((input_data.shape[0], self.output_dim, self.input_dim))\n",
        "        for i in range(input_data.shape[0]):\n",
        "          out[i] = np.transpose(self.W)\n",
        "        return out\n",
        "    def grad_b(self, input_data):\n",
        "        out = np.zeros((input_data.shape[0],self.b.shape[0],self.b.shape[0]))\n",
        "        for i in range(input_data.shape[0]):\n",
        "          out[i] = np.eye(self.b.shape[0])\n",
        "        return out\n",
        "    def grad_W(self, input_data):\n",
        "        third = self.input_dim*self.output_dim\n",
        "        # print((input_data.shape[0],self.output_dim,third))\n",
        "        out = np.zeros((input_data.shape[0],self.output_dim,third))\n",
        "        for i in range(input_data.shape[0]):\n",
        "          for j in range(self.output_dim):\n",
        "            tmp = []\n",
        "            tmp += [0] * j\n",
        "            data = input_data[i].tolist()\n",
        "            for k in range(len(data)-1):\n",
        "              tmp += [data[k]]\n",
        "              tmp += [0] * (self.output_dim-1)\n",
        "            tmp += [data[-1]]\n",
        "            tmp += [0] * (self.output_dim-j-1)\n",
        "            out[i,j] = np.array(tmp)\n",
        "        return out\n",
        "\n",
        "    def update_W(self, grad, learning_rate):\n",
        "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
        "\n",
        "    def update_b(self, grad,  learning_rate):\n",
        "        self.b -= learning_rate * np.mean(grad, axis=0)\n",
        "\n",
        "    def update_param(self, params_grad, learning_rate):\n",
        "        self.update_W(params_grad[0], learning_rate)\n",
        "        self.update_b(params_grad[1], learning_rate)\n",
        "\n",
        "    def grad_param(self, input_data):\n",
        "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'ReLU'\n",
        "    def forward(self, input_data):\n",
        "        return np.maximum(0, input_data)\n",
        "    def grad_x(self, input_data):\n",
        "        forw = (input_data > 0).astype('float32')\n",
        "        out = np.zeros((forw.shape[0],forw.shape[1],forw.shape[1]))\n",
        "        for i in range(forw.shape[0]):\n",
        "          out[i] = np.diag(forw[i])\n",
        "        return out\n",
        "\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Softmax'\n",
        "    def forward(self, input_data):\n",
        "        x = input_data - np.max(input_data, axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        sum_x = np.sum(x, axis=1, keepdims=True)\n",
        "        return x/sum_x\n",
        "    def grad_x(self, input_data):\n",
        "        y = self.forward(input_data)\n",
        "        out = np.zeros((y.shape[0],y.shape[1],y.shape[1]))\n",
        "        for i in range(y.shape[0]):\n",
        "          tmp = np.array([y[i]])\n",
        "          out[i] = np.diag(y[i]) - np.dot(np.transpose(tmp),tmp)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class FlattenLayer(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Flatten'\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        out = np.array(input_data[:,0,:,:])\n",
        "        shape = input_data.shape\n",
        "        return out.reshape(shape[0],-1)\n",
        "    def grad_x(self, input_data):\n",
        "        pass\n",
        "\n",
        "class MaxPooling(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'MaxPooling'\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        old_shape = input_data.shape\n",
        "        out = np.zeros((old_shape[0], old_shape[1], old_shape[2]//2,old_shape[3]//2))\n",
        "        for p in range(0, old_shape[0]):\n",
        "          for g in range(0, old_shape[1]):\n",
        "            for i in range(0,old_shape[2],2):\n",
        "              k = i // 2\n",
        "              for j in range(0,old_shape[3],2):\n",
        "                l = j // 2\n",
        "                res = -1000\n",
        "                res = np.maximum(res, input_data[p][g][i][j])\n",
        "                res = np.maximum(res, input_data[p][g][i+1][j])\n",
        "                res = np.maximum(res, input_data[p][g][i][j+1])\n",
        "                res = np.maximum(res, input_data[p][g][i+1][j+1])\n",
        "\n",
        "                out[p][g][k][l] = res # np.max(input_data[p][g][i:i+2][j:j+2]) не работает\n",
        "        return out\n",
        "        pass\n",
        "    def grad_x(self, input_data):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGKQ3o4Yy3lG"
      },
      "source": [
        "#### 1.2 Реализация свёрточного слоя и транспонированной свёртки (планируется)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-rSi6Tdy3lG"
      },
      "outputs": [],
      "source": [
        "class Conv2DLayer(Layer):\n",
        "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3,\n",
        "                 padding='same', stride=1, K_init=None, b_init=None):\n",
        "        # padding: 'same' или 'valid'\n",
        "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
        "        # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
        "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
        "        self.name = 'Conv2D'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.kernel = K_init\n",
        "        self.bias = b_init\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "        self.kernel = np.random.random((self.kernel_size, self.kernel_size, self.input_channels, self.output_channels))\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
        "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
        "        # Нужно заполнить Numpy-тензор out\n",
        "\n",
        "        # Согласование\n",
        "        if input_data.shape[1] != self.input_channels and (input_data.shape[2] - self.kernel_size) % self.stride != 0 and (input_data.shape[3]  - self.kernel_size) % self.stride != 0:\n",
        "          print('NE SOGLASOVANiE!!!')\n",
        "\n",
        "        batch = input_data.shape[0]\n",
        "        inch = input_data.shape[1]\n",
        "        h = input_data.shape[2]\n",
        "        w = input_data.shape[3]\n",
        "        kh = self.kernel_size\n",
        "        kw = self.kernel_size\n",
        "        pad = 0\n",
        "\n",
        "        if self.padding == 'valid':\n",
        "          output_height = (h - self.kernel_size) // self.stride + 1\n",
        "          output_width = (w - self.kernel_size) // self.stride + 1\n",
        "        else:\n",
        "          output_height = h\n",
        "          output_width = w\n",
        "\n",
        "        out = np.array((batch, self.output_channels, output_height, output_width))\n",
        "        # padded_input = np.pad(input_data, pad, mode='constant')\n",
        "\n",
        "        if self.padding == 'valid':\n",
        "          for bch in batch:\n",
        "            for i in range(0, output_height):\n",
        "                for j in range(0, output_width):\n",
        "                    out[bch, i, j] = np.sum(np.dot(input_data[bch,:,i*self.stride:i*self.stride+kh,\n",
        "                                          j*self.stride:j*self.stride+kw], self.kernel))\n",
        "\n",
        "        return out\n",
        "\n",
        "    def grad_x(self):\n",
        "        pass\n",
        "    def grad_kernel(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAFXgXBVy3lH"
      },
      "outputs": [],
      "source": [
        "class Conv2DTrLayer(Layer):\n",
        "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3,\n",
        "                 padding=0, stride=1, K_init=None, b_init=None):\n",
        "        # padding: число (сколько отрезать от модифицированной входной карты)\n",
        "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
        "        # stride - одно число (коэффициент расширения)\n",
        "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
        "        self.name = 'Conv2DTr'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.kernel = K_init\n",
        "        self.bias = b_init\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "    def forward(self, input_data):\n",
        "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
        "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
        "        # Нужно заполнить Numpy-тензор out\n",
        "        out = np.empty([])\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        pass\n",
        "    def grad_x(self):\n",
        "        pass\n",
        "    def grad_kernel(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl5mB1oHy3lH"
      },
      "source": [
        "#### 1.4 Теперь настало время теста."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LBNZusdy3lH"
      },
      "source": [
        "#### Чтение данных"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Раскомментировать, если установлена старая версия библиотеки\n",
        "!pip install np_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhWEJA0UgUlu",
        "outputId": "ed0e4d6d-66db-45b7-c8a6-a3a127ca1497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from np_utils) (2.0.2)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56437 sha256=9b95b78eb29f8a02980cf9f13e54062e013d56b1d55034cbc0f6ef109ed7ce5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/0d/33/eaa4dcda5799bcbb51733c0744970d10edb4b9add4f41beb43\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5yWfdViy3lH",
        "outputId": "73edbbc7-f3a4-422d-8f5d-262d1c65a9fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)  # for reproducibility\n",
        "import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "Y_train = to_categorical(y_train, 10)\n",
        "Y_test = to_categorical(y_test, 10)\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLGgwpuGy3lH"
      },
      "source": [
        "#### Подготовка моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6xH3-6_y3lI",
        "outputId": "46aadfd0-b574-41f4-e6ec-becdedc2f409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.0\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
        "\n",
        "print(keras.__version__)\n",
        "\n",
        "def get_keras_model():\n",
        "    input_image = Input(shape=(1, 28, 28))\n",
        "    pool1 = MaxPooling2D(pool_size=(2,2), data_format='channels_first')(input_image)\n",
        "    flatten = Flatten()(pool1)\n",
        "    dense1 = Dense(10, activation='softmax')(flatten)\n",
        "    model = Model(inputs=input_image, outputs=dense1)\n",
        "\n",
        "    from keras.optimizers import Adam, SGD\n",
        "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=sgd,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.25,\n",
        "                        batch_size=32, epochs=2, verbose=1)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6sUeO-By3lI"
      },
      "outputs": [],
      "source": [
        "def get_our_model(keras_model):\n",
        "    maxpool = MaxPooling()\n",
        "    flatten = FlattenLayer()\n",
        "    dense = DenseLayer(196, 10, W_init=keras_model.get_weights()[0],\n",
        "                       b_init=keras_model.get_weights()[1])\n",
        "    softmax = Softmax()\n",
        "    net = Network([maxpool, flatten, dense, softmax])\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uub14Vnqy3lI",
        "outputId": "793a8484-425c-482a-d424-0385c3a79296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7662 - loss: 0.8570 - val_accuracy: 0.8926 - val_loss: 0.3806\n",
            "Epoch 2/2\n",
            "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8913 - loss: 0.3867 - val_accuracy: 0.9008 - val_loss: 0.3447\n"
          ]
        }
      ],
      "source": [
        "keras_model = get_keras_model()\n",
        "our_model = get_our_model(keras_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuDz_Qbsy3lI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e59be2-bc42-41f0-fec2-5934c35f83be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "keras_prediction = keras_model.predict(X_test)\n",
        "our_model_prediction = our_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p65FOEAUy3lI",
        "outputId": "d64e1be3-7a81-483f-f0de-9a55a42eb7ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "if np.sum(np.abs(keras_prediction - our_model_prediction)) < 0.01:\n",
        "    print('Test PASSED')\n",
        "else:\n",
        "    print('Something went wrong!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.abs(keras_prediction - our_model_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNvBmlNYIBZr",
        "outputId": "c29bbc6b-4b03-4e3d-9304-4ea81ce57047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.000981925348501486)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p23hovfny3lI"
      },
      "source": [
        "### 2. Вычисление производных по входу для слоёв нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0-9884Qy3lJ"
      },
      "source": [
        "#### 2.1  Реализация метода forward для класса CrossEntropy\n",
        "Формула выглядит следующим образом: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
        "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q50CpL31y3lJ"
      },
      "outputs": [],
      "source": [
        "class CrossEntropy(object):\n",
        "    def __init__(self, eps=0.00001):\n",
        "        self.name = 'CrossEntropy'\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input_data, labels):\n",
        "        out = np.zeros(input_data.shape[0])\n",
        "        for i in range(input_data.shape[0]):\n",
        "          labels[i] = np.clip(labels[i], self.eps, 1 - self.eps)\n",
        "          out[i] = -np.sum(labels[i] * np.log(input_data[i]))\n",
        "        return out\n",
        "    def calculate_loss(self,input_data, labels):\n",
        "        return self.forward(input_data, labels)\n",
        "\n",
        "    def grad_x(self, input_data, lables):\n",
        "        for i in range(input_data.shape[0]):\n",
        "            lables[i] = np.clip(lables[i], self.eps, 1 - self.eps)\n",
        "        return -lables / input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nlxl_toy3lJ"
      },
      "source": [
        "#### 2.2  Реализация метода grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnBLQ2u6y3lJ"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5MY2duTy3lJ",
        "outputId": "d858eb3a-5b37-4d41-d177-bb923bb2de9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_diff_net(net, x, labels):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
        "        right_answer.append(diff)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_net(net):\n",
        "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
        "    labels = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
        "    num_grad = numerical_diff_net(net, x, labels)\n",
        "    grad = net.grad_x(x, labels)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "loss = CrossEntropy()\n",
        "test_net(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1BUjB0ay3lJ"
      },
      "source": [
        "#### 2.3  Реализация метода grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZmBkhVXy3lJ"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImC_6ZF8y3lJ",
        "outputId": "1006418b-0c6a-4902-81dc-742d21ff0fee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_diff_layer(layer, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_layer(layer):\n",
        "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
        "    num_grad = numerical_diff_layer(layer, x)\n",
        "    grad = layer.grad_x(x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "layer = Softmax()\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zxLCO89y3lK"
      },
      "source": [
        "#### 2.4  Реализация метода grad_x для классов ReLU и DenseLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WmxQn-py3lK",
        "outputId": "d6eefe4f-c6b4-45ed-c570-5c96e256b2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "layer = ReLU()\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j66AZAhoy3lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599f60bf-d8bf-483c-87c2-d2c872a915d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "layer = DenseLayer(3,4)\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzbX1ixEy3lK"
      },
      "source": [
        "#### 2.5 Для класса Network реализуется метод grad_x, который должен осуществлять взятие производной от лосса по входу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWWDLVBOy3lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4b689e-41b4-46a3-81c8-5fd3a16a1600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
        "test_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT9WEcOUy3lK"
      },
      "source": [
        "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOYT1ihny3lP"
      },
      "source": [
        "#### 3.1  Реализация функции grad_b и grad_W."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWAFwF8cy3lP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f97619b-baa9-4995-f0a9-4e5d9aeac333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad_b(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(b)):\n",
        "        delta = np.zeros(b.shape)\n",
        "        delta[i] = eps\n",
        "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
        "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
        "        diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_b():\n",
        "    input_size = 3\n",
        "    output_size = 4\n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((output_size,))\n",
        "    x = np.random.random((2, input_size))\n",
        "\n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_b(x)\n",
        "\n",
        "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_b()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr3mJ48-y3lQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd30edc-0369-4e72-dc37-87f1a785ba8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad_W(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            delta = np.zeros(W.shape)\n",
        "            delta[i, j] = eps\n",
        "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
        "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
        "            diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "            right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_W():\n",
        "    input_size = 3\n",
        "    output_size = 4\n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((4,))\n",
        "    x = np.random.random((2, input_size))\n",
        "\n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_W(x)\n",
        "\n",
        "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_W()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXRtmKPQy3lQ"
      },
      "source": [
        "#### 3.2 Полная реализация метода обратного распространения ошибки в функции train_step класса Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prhEapjDy3lQ"
      },
      "source": [
        "Сначала напишем реализацию функцим Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
        "После чего, имея список градиентов, напишем функцию обновления параметров для каждого слоя.\n",
        "\n",
        "Далее пишется тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Z1b8-Py3lQ"
      },
      "source": [
        "#### 3.3 Запустим обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR2bVtTdy3lQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391c39a5-6eac-46d9-ec56-ba17f09d6101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:11<00:00,  7.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 epoch: val 0.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:13<00:00,  7.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 epoch: val 0.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:12<00:00,  7.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 epoch: val 0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:11<00:00,  7.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 epoch: val 0.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 937/937 [02:13<00:00,  7.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 epoch: val 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::3], Y_train[::3], validation_split=0.25,\n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqy0J1-jy3lQ"
      },
      "outputs": [],
      "source": [
        "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::6], Y_train[::6], validation_split=0.25,\n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BifYYtQy3lR"
      },
      "source": [
        "#### 3.5 Посмотрим на возможность нашей нейросети обучать более глубокие нейронные сети"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сначала выдает ошибку деления на ноль, но потом все норм. На больше эпох не хватило времени :((\n",
        "net = Network([DenseLayer(784, 50), ReLU(),\n",
        "               DenseLayer(50, 40), ReLU(),\n",
        "               DenseLayer(40, 20), ReLU(),\n",
        "               DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX, Y_train, validation_split=0.25,\n",
        "            batch_size=128, nb_epoch=5, learning_rate=0.001)"
      ],
      "metadata": {
        "id": "ORULpjg50u6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}